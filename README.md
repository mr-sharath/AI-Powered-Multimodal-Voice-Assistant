ğŸš€ Excited to share my latest project: AI-Powered Multimodal Voice Assistant!

As part of my work at the University of Houston's AI & Computer Vision Lab, I developed an end-to-end AI system that combines speech recognition, image analysis, and text-to-speech synthesis to create a seamless voice assistant experience.

Hereâ€™s what it does:
ğŸ¤ Speech Recognition: Uses OpenAIâ€™s Whisper model to transcribe spoken audio into text.
ğŸ–¼ï¸ Image Analysis: Leverages Microsoftâ€™s GIT-base model to generate captions and answer questions about uploaded images.
ğŸ—£ï¸ Text-to-Speech: Converts text responses back into speech using Googleâ€™s gTTS.

The system is built with Gradio for an intuitive user interface, allowing users to upload images, ask questions via voice, and receive both text and audio responses in under 2 seconds!

This project was a fantastic opportunity to work under the guidance of Professor Justin and collaborate with 2 PhD students to integrate cutting-edge AI models like LLaVA and Whisper. Itâ€™s amazing to see how AI can bridge the gap between visual and auditory interactions, enabling natural language Q&A about visual content.

Check out the details below, and feel free to reach out if youâ€™re interested in AI, computer vision, or multimodal systems!

#AI #ComputerVision #MultimodalAI #VoiceAssistant #OpenAI #Whisper #LLaVA #Gradio #MachineLearning #ArtificialIntelligence

https://mr-sharath.github.io/
