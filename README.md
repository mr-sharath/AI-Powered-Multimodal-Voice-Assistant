🚀 Excited to share my latest project: AI-Powered Multimodal Voice Assistant!

As part of my work at the University of Houston's AI & Computer Vision Lab, I developed an end-to-end AI system that combines speech recognition, image analysis, and text-to-speech synthesis to create a seamless voice assistant experience.

Here’s what it does:
🎤 Speech Recognition: Uses OpenAI’s Whisper model to transcribe spoken audio into text.
🖼️ Image Analysis: Leverages Microsoft’s GIT-base model to generate captions and answer questions about uploaded images.
🗣️ Text-to-Speech: Converts text responses back into speech using Google’s gTTS.

The system is built with Gradio for an intuitive user interface, allowing users to upload images, ask questions via voice, and receive both text and audio responses in under 2 seconds!

This project was a fantastic opportunity to work under the guidance of Professor Justin and collaborate with 2 PhD students to integrate cutting-edge AI models like LLaVA and Whisper. It’s amazing to see how AI can bridge the gap between visual and auditory interactions, enabling natural language Q&A about visual content.

Check out the details below, and feel free to reach out if you’re interested in AI, computer vision, or multimodal systems!

#AI #ComputerVision #MultimodalAI #VoiceAssistant #OpenAI #Whisper #LLaVA #Gradio #MachineLearning #ArtificialIntelligence

https://mr-sharath.github.io/
